!pip install keras-tuner --upgrade
#Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import math

#loading the data
data = pd.read_csv("/content/forest_data_combined.csv")
print(data.head())
print(data.columns)

#Selecting the feature to predict
target_col = ['Forest Area (sq. km)','Mean Rainfall (mm)','Mean Temperature (°C)' ]
data = data[target_col].dropna()

#Normalize the data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

# Sequence creator
def create_multivariate_sequences(dataset, target_index, time_steps=60):
    x, y = [], []
    for i in range(time_steps, len(dataset)):
        x.append(dataset[i-time_steps:i])
        y.append(dataset[i, target_index])
    return np.array(x), np.array(y)

features = target_col
target_index = features.index('Forest Area (sq. km)')
X, y = create_multivariate_sequences(scaled_data, target_index, time_steps=60)

#split data into testing and training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)

#Define the GRU Model Builder
def build_model(hp):
    model = Sequential()

    model.add(GRU(
        units=hp.Int('gru_units_1', min_value=64, max_value=256, step=32),
        return_sequences=True,
        input_shape=(X_train.shape[1], X_train.shape[2])
    ))
    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1)))

    model.add(GRU(units=hp.Int('gru_units_2', min_value=64, max_value=256, step=32)))
    model.add(Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1)))

    model.add(Dense(1))

    model.compile(
        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='mean_squared_error'
    )

    return model

# Set Up the Tuner
tuner = kt.RandomSearch(build_model,objective='val_loss',max_trials=10,executions_per_trial=1,directory='my_gru_tuner',project_name='forest_cover_gru')

#Run the Tuning
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
tuner.search(X_train, y_train,epochs=50,validation_data=(X_test, y_test),callbacks=[early_stop],verbose=1)

# Get and Train the Best GRU Model
best_model = tuner.get_best_models(num_models=1)[0]
best_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=[early_stop])

# Make Predictions
predictions = best_model.predict(X_test)

# Inverse Transform Predictions and Actuals
def inverse_transform_target_only(scaled_column, target_index, total_features):
    padded = np.zeros((len(scaled_column), total_features))
    padded[:, target_index] = scaled_column.flatten()
    return scaler.inverse_transform(padded)[:, target_index]

predicted = inverse_transform_target_only(predictions, target_index, len(features))
actual = inverse_transform_target_only(y_test.reshape(-1, 1), target_index, len(features))

#Plot Results
plt.figure(figsize=(12, 6))
plt.plot(actual, label='Actual Forest Cover')
plt.plot(predicted, label='Predicted Forest Cover (GRU)')
plt.title('GRU with Keras Tuner - Forest Cover Forecast')
plt.xlabel('Time')
plt.ylabel('Forest Cover')
plt.legend()
plt.grid(True)
plt.show()

#Evaluate Accuracy Metrics

mae = mean_absolute_error(actual, predicted)
mse = mean_squared_error(actual, predicted)
rmse = math.sqrt(mse)
r2 = r2_score(actual, predicted)

print("Evaluation Metrics:")
print(f"MAE  (Mean Absolute Error): {mae:.4f}")
print(f"MSE  (Mean Squared Error):  {mse:.4f}")
print(f"RMSE (Root Mean Squared):   {rmse:.4f}")
print(f"R²   (R-squared score):     {r2:.4f}")
