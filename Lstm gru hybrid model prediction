# Install necessary packages
!pip install keras-tuner --quiet
!pip install tensorflow --quiet

# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import keras_tuner as kt
import math

# Load data
sheet_id = "1vs0k8YN1bI1Q9Zg6exaPpEVoVSXgpXRrRn9VTsCIdgY"
sheet_name = "Sheet1"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"
data = pd.read_csv(url)

# Expected columns
columns_needed = [
    'Forest Name', 'Year', 'Forest Area (sq. km)', 'NDVI Mean',
    'Forest Density (%)', 'Change Magnitude (%)',
    'Mean Rainfall (mm)', 'Mean Temperature (Â°C)'
]

data = data[columns_needed]
forests = data['Forest Name'].unique()
time_steps = 30

# Sequence generator for multivariate input
def create_sequences(df, target_idx, time_steps):
    X, y = [], []
    for i in range(time_steps, len(df)):
        X.append(df[i - time_steps:i])
        y.append(df[i, target_idx])
    return np.array(X), np.array(y)

# Hybrid LSTM-GRU model
def build_model(hp):
    model = Sequential()
    model.add(LSTM(
        units=hp.Int("lstm_units", 32, 128, step=32),
        return_sequences=True,
        input_shape=(time_steps, n_features)
    ))
    model.add(Dropout(hp.Float("dropout_1", 0.1, 0.5, step=0.1)))
    model.add(GRU(
        units=hp.Int("gru_units", 32, 128, step=32),
        return_sequences=False
    ))
    model.add(Dropout(hp.Float("dropout_2", 0.1, 0.5, step=0.1)))
    model.add(Dense(1))
    model.compile(
        loss='mse',
        optimizer=Adam(hp.Choice('lr', [1e-2, 1e-3, 1e-4]))
    )
    return model

# Inverse transform helper
def inverse_transform_forest(preds_scaled, feature_idx, scaler, base_shape):
    full_data = np.zeros((len(preds_scaled), base_shape))
    full_data[:, feature_idx] = preds_scaled
    return scaler.inverse_transform(full_data)[:, feature_idx]

# Loop over each forest
for forest in forests:
    print(f"\nProcessing: {forest}")
    df_forest = data[data['Forest Name'] == forest].sort_values(by='Year')
    df_forest.drop(columns=['Forest Name', 'Year'], inplace=True)

    if len(df_forest) < time_steps + 1:
        print(f"Not enough data ({len(df_forest)} rows). Padding...")

    # Normalize
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(df_forest)
    n_features = scaled.shape[1]
    target_idx = list(df_forest.columns).index('Forest Area (sq. km)')

    # Prepare sequences
    X, y = create_sequences(scaled, target_idx, time_steps)

    # Handle extremely small datasets
    if len(X) < 2:
        print(f"Skipping training, using most recent sequence to predict future.")
        X_future = scaled[-time_steps:].reshape(1, time_steps, n_features)
        predicted_scaled = np.repeat(X_future, 30, axis=0)
    else:
        # Train-test split (70-30)
        split = int(len(X) * 0.7)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]

        # Hyperparameter tuning
        tuner = kt.RandomSearch(
            build_model,
            objective="val_loss",
            max_trials=5,
            directory="tuner_dir",
            project_name=f"{forest.replace(' ', '_')}_multivariate"
        )
        tuner.search(X_train, y_train, validation_data=(X_test, y_test),
                     epochs=30, verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])

        best_model = tuner.get_best_models(num_models=1)[0]
        best_model.fit(X_train, y_train, validation_data=(X_test, y_test),
                       epochs=30, verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])

        # ===== Evaluate and Print Metrics =====
        y_pred_scaled = best_model.predict(X_test, verbose=0).flatten()
        y_test_scaled = y_test

        y_pred = inverse_transform_forest(y_pred_scaled, target_idx, scaler, n_features)
        y_true = inverse_transform_forest(y_test_scaled, target_idx, scaler, n_features)

        mae = mean_absolute_error(y_true, y_pred)
        rmse = math.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)

        print(f"\nðŸ“Š Metrics for {forest}:")
        print(f"MAE:  {mae:.2f}")
        print(f"RMSE: {rmse:.2f}")
        print(f"RÂ²:   {r2:.4f}")

        # ===== Forecast next 30 years =====
        last_sequence = scaled[-time_steps:]
        predictions_scaled = []
        current_input = last_sequence.copy()
        for _ in range(30):
            input_ = current_input.reshape(1, time_steps, n_features)
            pred = best_model.predict(input_, verbose=0)[0][0]
            next_input = current_input[1:]
            new_row = current_input[-1].copy()
            new_row[target_idx] = pred
            current_input = np.vstack([next_input, new_row])
            predictions_scaled.append(pred)

        predicted_scaled = np.array(predictions_scaled)

    # Inverse transform predictions
    future_years = list(range(2021, 2021 + 30))
    actual_years = data[data['Forest Name'] == forest]['Year'].values
    actual_forest_area = df_forest['Forest Area (sq. km)'].values

    predicted_forest_area = inverse_transform_forest(predicted_scaled, target_idx, scaler, n_features)

    # Plot
    plt.figure(figsize=(10, 5))
    plt.plot(actual_years, actual_forest_area, label='Historical', marker='o')
    plt.plot(future_years, predicted_forest_area, label='Predicted (2021â€“2050)', linestyle='--', marker='x')
    plt.title(f"Forest Area Prediction: {forest}")
    plt.xlabel("Year")
    plt.ylabel("Forest Area (sq. km)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Print future predictions
    print("\nPredicted Forest Area from 2021 to 2050:")
    for year, area in zip(future_years, predicted_forest_area):
        print(f"{year}: {area:.2f} sq. km")
